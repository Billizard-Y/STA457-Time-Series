---
title: "STA457 HW1"
author: "Depeng Ye 1002079500"
date: "30/09/2019"
output: pdf_document
---
```{r, include=FALSE}
library(ggplot2)
library(Rlab)
```

\section*{Problem 1}
\subsection*{a)}Since all $r_i$ are i.i.d. random variables that follow $N(\mu, \sigma^2)$.  
By the additivity of Normality, $r_t(3) = r_t + r_{t-1} + r_{t-2}$ should follow Normal distribution as well with the mean of $E[r_t(3)] = E[r_t + r_{t-1} + r_{t-2}] = E[r_t] + E[r_{t-1}] + E[r_{t-2}] = \mu + \mu + \mu = 3\mu$, and variance of $Var[r_t(3)] = Var[r_t + r_{t-1} + r_{t-2}] = Var[r_t] + Var[r_{t-1}] + Var[r_{t-2}] = \sigma^2 + \sigma^2 + \sigma^2 = 3\sigma^2$.  
Hence $r_t(3) \sim N(3\mu, 3\sigma^2)$.
  
\subsection*{b)}
\begin{align*}
Cov(r_t(k), \ r_t(k+l)) &= Cov(r_t + r_{t-1} + \dots + r_{t-k+1}, r_t(k+l)) \\
&= Cov(r_t, \ r_t(k+l)) + Cov(r_{t-1},\  r_t(k+l)) + \dots + Cov(r_{t-k+1}, \  r_t(k+l)) \\
&= Cov(r_t, \ r_t) + Cov(r_t, \ r_{t-1}) + \dots + Cov(r_t, \  r_{t-k-l+1}) + Cov(r_{t-1}, \ r_t) +\\
& \ \ \ \ Cov(r_{t-1}, \ r_{t-1}) + \dots + Cov(r_{t-1}, \ r_{t-k-l+1}) + \cdots \\
&= Var(r_t) + Var(r_{t-1}) + \dots + Var(r_{t-k+1}) \\
&= k\sigma^2
\end{align*}


\section*{Problem 2}
\subsection*{a)}
Simulating the annual price of the first 10 years using rnorm() function as random error and the results are shown in the table below. 
```{r, echo = T}
set.seed(2)
p0 <- 20
years <- 10
mu <- 0.03
sigma <- 0.005
error <- rnorm(10, mu, sigma)
current_price <- data.frame(exp(log(p0) + cumsum(error)))
year <- data.frame(c('P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10'))
full_list <- cbind(year,round(current_price, digits = 2))
colnames(full_list) <- c("Year", "Price")
knitr::kable(full_list, digits = 2, caption = "Price of the asset for nest 10 years", 
             col.names = c('year', 'price'))
```

The plot is shown as follows:

```{r echo = F, fig.width= 5, fig.height= 3, fig.align='center'}
ggplot(full_list, aes(x = c(1:10), y = full_list$Price), size = 10) +
  geom_line(col = "tomato") + labs(title = "Price Random Walk of 10 years", x = "Year", y = "Price") + 
  geom_point(col = "orange") + 
  theme_classic(base_size = 10) + 
  scale_x_continuous(breaks = 1:10) + scale_y_continuous(breaks = round(full_list$Price))

```

\subsection*{b)}
Simulate $P_{10}$ 2000 times using rnorm() function as random error. The results are not shown because they will take too much of space. The expected value is calculated using mean() fucniton. 
```{r}
P_10_vec = c()
for (i in 1 : 2000){
  e = rnorm(10, mu, sigma) 
  P_10 = exp(log(p0) + sum(e))
  P_10_vec = append(P_10_vec, P_10)
  i = i + 1
}
print(mean(P_10_vec), digits = 2)
```
The expected value of $P_{10}$ is $E[P_{10}] = 27$.  

\subsection*{c)}
My simulated result in a) ($P_{10} = 27.28$) and slightly larger than the actual expected value of $E[P_{10}] = 27$ in part b).  
\newline
The reason is that the effect of the random error is reduced by the large number of data set. When there is 2000 estimates of $P_{10}$, the mean of the random error used (rnorm) tend to become 0. Therefore, the expected value of $P_{10}$ is more accurate in part b).


\section*{Problem 3}
When compared with the examples shown in the lecture, exchanging horizontal and vertical axes will invert the convexity of the curve in Q-Q plot. Namely, the concave pattern in lecture's plot will be a convex pattern in this question and vice versa. 
\subsection*{a)}
The convex pattern in the plot required in this question interprets a right-skewed distribution of sample. For example, Chi-square distribution.
\subsection*{b)}
The concave pattern in the plot required in this question interprets a left-skewed distribution of sample.
\subsection*{c)}
The convex-concave pattern in the plot required in this question interprets a light tailed pattern in the sample distribution. 
\subsection*{d)}
The concave-convex pattern in the plot required in this question interprets a heavy tailed pattern in the sample distribution. 


\section*{Problem 4}
\subsection*{a)}
Because $X_1$, $X_2$, $\dots$ is a lognormal geometric random walk with parameter $(\mu, \sigma^2)$, we have $X_2 = X_0 \exp(r_1 + r_2)$. 
It is easy to interpret from the above equation that  
\begin{align*}
& \ \ \ \ \   X_2 = X_0 \exp(r_1 + r_2) \\
&\Rightarrow \frac{X_2}{X_0} = \exp(r_1 + r_2) \\
&\Rightarrow \log(\frac{X_2}{X_0}) = r_1 + r_2
\end{align*}
which implies $\log\frac{X_2}{X_0}$ follows $N(2\mu, 2\sigma^2)$. Because $(r_1 + r_2) \sim N(2\mu, 2\sigma^2)$ form Problem 1.     
Try to solve this question based on Standard Normal distribution. 
\begin{align*}
P(X_2 > 1.5 X_0) &= P(\frac{X_2}{X_0} > 1.5) \\
&= P(\log\frac{X_2}{X_0} > \log1.5) \\
&= P(\frac{\log\frac{X_2}{X_0} - 2\mu}{\sqrt{2}\sigma} > \frac{\log1.5-2\mu}{\sqrt{2} \sigma})\\
&= P(Z > \frac{\log 1.5 - 2\mu}{\sqrt{2} \sigma}) \\
&= 1 - P(Z < \frac{\log 1.5 - 2\mu}{\sqrt{2} \sigma}) \\
\end{align*}
Hence, $$P(X_2 > 1.5 X_0) = 1 - \Phi(Z < \frac{\log 1.5 - 2\mu}{\sqrt{2} \sigma})$$
where $\Phi$ is the cumulative density funciton of standardized normal distribution.

\subsection*{b)}
Want to find the 0.8 quantile of $X_k$ for all $k$, need to first know the distribution of $X_k$.  
Because $X_1, X_2, X_3, \dots$ are a lognormal geometric random walk with parameters $(\mu, \sigma^2)$ and $X_k = X_0 \exp(r_1 + r_ 2 + \dots + r_k)$. With similar interpretation as part a), should have $\log \frac{X_k}{X_0} \sim N(k\mu, k\sigma^2)$  
Assume that the 0.8 quantile of $X_k$ is Q, by definition of quantile: $$P(X_k < Q) = 0.8$$
Take $\log$ on both sides and then standardize the left side:  
\begin{align*}
& P(X_k < Q) = 0.8\\
&\Rightarrow P(\log X_k < \log Q) = 0.8 \\
&\Rightarrow P(\frac{\log X_k - k \mu}{\sqrt{k} \sigma} < \frac{\log Q - k\mu}{\sqrt{k} \sigma}) = 0.8 \\
\end{align*}
Notice that only $\log \frac{X_k}{X_0}$ follows normal distribution, try to substract $\frac{\log X_0}{\sqrt{k} \sigma}$ from both sides of the inequality inside of $P()$: 
\begin{align*}
&\Rightarrow P(\frac{\log X_k - \log X_0 - k \mu}{\sqrt{k} \sigma} < \frac{\log Q - \log X_0 - k\mu}{\sqrt{k} \sigma}) = 0.8\\
&\Rightarrow P(\frac{\log \frac{X_k}{X_0} - k \mu}{\sqrt{k} \sigma} < \frac{\log Q - \log X_0 - k\mu}{\sqrt{k} \sigma}) = 0.8\\
\end{align*}
where $\frac{\log \frac{X_k}{X_0} - k \mu}{\sqrt{k} \sigma}$ is the standardized $Z$ of $\log \frac{X_k}{X_0}$, in this way we can now check the Z-table.  
By checking the Z-table, 
\begin{align*}
\frac{\log Q - \log X_0 - k\mu}{\sqrt{k} \sigma} &= 0.84162 \\
\log Q - \log X_0 - k \mu &= 0.84162 \sqrt{k} \sigma \\
Q &= \exp( \log X_0 + k\mu + 0.84162 \sqrt{k} \sigma) \\
Q &= X_0 + \exp(k\mu + 0.84162 \sqrt{k} \sigma)
\end{align*}
Hence, the 0.8 quantile of $X_k$ is $Q = X_0 + \exp(k\mu + 0.84162 \sqrt{k} \sigma)$. 

\subsection*{c)}
Want to find $E[X_k^2]$ as a function of $k$, need the second moment density function of $N(k\mu, k\sigma^2)$ given that we know $\log \frac{X_k}{X_0} \sim N(k\mu, k\sigma^2)$ i.e. $\log X_k \sim N(\log X_0 + k\mu, k \sigma^2)$.  
From the distribution we have the density function: 
$$f(x_k) = \frac{1}{\sqrt{2 \pi k}\sigma x_k}\exp \bigg(- \frac{(\log x_k - \log x_0 - k \mu)^2}{2 k\sigma^2}\bigg)$$
Considering the second moment, the expectation of $X^2_k$ is :
$$E[X_k^2] = \int_{-\infty}^{\infty}X_k^2 \frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{(\log x_k - \log x_0 - k\mu)^2}{2 k \sigma ^2}\bigg) d(\sum_{i = 1} ^ k{r_i})$$
Now consider $X_k = X_0 \exp(r_k + r_{k-1} + \dots + r_2 + r_1)$. Using the notation in Problem 1 we have: $r_k(k) = r_k + r_{k-1} + \dots + r_2 + r_1$.  
Then $X_k$ can be written as $X_0 \exp(r_k(k))$, and $\log X_k = \log X_0 + r_k(k)$. \\
\begin{align*}
E[X_k^2] &= \int_{-\infty}^{\infty}X_k^2 \frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{(\log x_k - \log x_0 - k\mu)^2}{2 k \sigma ^2}\bigg) d(\sum_{i = 1} ^ k{r_i}) \\
&= \int_{- \infty}^{\infty} X_0^2 \exp(r_k(k))^2 \frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{[\log x_0 + r_k(k) - (\log x_0 + k \mu)]^2}{2k \sigma^2} \bigg) dr_k(k) \\
&= X_0^2 \exp(2k\mu + \frac{4k\sigma^2}{2}) \int_{-\infty}^{\infty}\frac{1}{\sqrt{2 \pi k} \sigma} \exp\bigg(-\frac{[r_k(k) - (k\mu + 2k\sigma^2)]^2}{2k\sigma^2}\bigg)dr_k(k) \\
\end{align*}
Notice that inside the integral is the pdf of normal distribution $N(k\mu + 2k\sigma^2, k\sigma^2)$, therefore the integral should be equal to 1.  
Thus the expectation of $X_k^2$ is :
$$E[X_k^2] = X_0^2 e^{2k\mu + \frac{4k\sigma^2}{2}} = X_0^2 e^{2k\mu + 2k\sigma^2} $$

\subsection*{d)}
To calculate the variance of $X_k$, use the formula $Var(X_k) = E[X_k^2] - E[X_k]^2$. We have already calcuated $E[X_k^2]$ in part c), and now only need to find $E[X_k]$.  
Similarly, use the pdf method to find expectation fo $X_k$:
\begin{align*}
E[X_k] &= \int_{-\infty}^{\infty}X_k \frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{(\log x_k - \log x_0 - k\mu)^2}{2 k \sigma ^2}\bigg) d(\sum_{i = 1} ^ k{r_i}) \\
&= \int_{- \infty}^{\infty} X_0 \exp(r_k(k)) \frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{[\log x_0 + r_k(k) - (\log x_0 + k \mu)]^2}{2k \sigma^2} \bigg) dr_k(k)\\
&= X_0 \exp \bigg(k\mu + \frac{k\sigma^2}{2}\bigg) \int_{-\infty}^{\infty}\frac{1}{\sqrt{2 \pi k}\sigma} \exp \bigg(-\frac{[r_k(k) - (k\mu + k \sigma^2)]^2}{2k\sigma^2}\bigg)dr_k(k)\\
&= X_0 e^{k\mu + \frac{k\sigma^2}{2}}\\
\end{align*}
Hence together with te result from part c):
\begin{align*}
Var(X_k) &=  E[X_k^2] - E[X_k]^2 \\
&= X_0^2 e^{2k\mu + 2k\sigma^2} - (X_0 e^{k\mu + \frac{k\sigma^2}{2}})^2 \\
&= X_0^2 \big(e^{2k\mu +2k\sigma^2} -e^{2k\mu + k\sigma^2}\big)\\
&= X_0^2 e^{2k\mu + k\sigma^2} (e^{k\sigma^2} - 1)
\end{align*}

\newpage
\section*{Problem 5}
Want to find the MLE of $\sigma^2$ when $\mu$ is known with $Y_1, Y_2, \dots, Y_n$ i.i.d. $N(\mu, \sigma^2)$.  
The pdf of $Y_i$ is $f (x) = \frac{1}{\sqrt{2 \pi}\sigma}\exp\big(-\frac{(y - \mu)^2}{2 \sigma ^2}\big)$.  
First find the likelihood:  
$$L(\mu, \sigma^2) = \prod_{i =1}^nf(y_i)$$
The find the log likelihood:  
\begin{align*}
l(\mu, \sigma^2) &= \log(L(\mu, \sigma^2)) \\
&= \sum_{i = 1}^n \log f(y_i) \\
&= -\frac{n}{2} \log 2\pi - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i- \mu)^2\\
\end{align*}
MLE of $\sigma^2$ given that $\mu$ is known:  
Let: 
$$0 = \frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i = 1}^n (y_i - \mu)^2$$
$$\Rightarrow \widehat{\sigma}^2 = n^{-1} \sum_{i=1}^n(Y_i - \mu)^2$$
Hence, the MLE of $\sigma^2$ when $\mu$ is known is: $\widehat{\sigma}^2 = n^{-1} \sum_{i=1}^n(Y_i - \mu)^2$.
\begin{flushright} $\blacksquare $ \end{flushright}

\newpage
\section*{Problem 6}
\subsection*{a)}
Want to find the mean adn variance of $Z =  YX_1 + (1-Y)X_2$.
\newline
First do the expectation part:
\begin{align*}
E[Z] &= E[YX_1 + (1-Y)X_2] \\
&= E[YX_1] + E[(1-Y)X_2] \\
&= E[Y]E[X_1] + E[1-Y]E[X_2]\\
&= E[Y]E[X_1] + (E[1] - E[Y])E[X_2]
\end{align*}
Since $X_1$ and $X_2$ follows $N(0, \sigma^2)$ and $Y$ follows $Bernoulli(p)$. $E[X_1] = E[X_2] = 0$, $E[Y] = p$.  
The above equation becomes:
$$E[Z] = p \times 0 + (1 - p) \times 0 = 0$$
\newline 
Then consider $Var(Z)$:
\begin{align*}
Var(Z) &= Var(YX_1 + (1-Y)X_2) \\
&= Var(YX_1) + Var((1-Y)X_2) + 2Cov(YX_1, (1-Y)X_2) \\
\end{align*}
because $X_1$, $X_2$ and $Y$ are independent, with the formula $Var(XY) = E[X]^2Var(Y) + E[Y]^2Var(X) + Var(X)Var(Y)$:
\begin{align*}
Var(Z) &= E[X_1]^2Var(Y) + E[Y]^2Var(X_1) + Var(Y)Var(X_1) + E[X_2]^2Var(Y) + E[1-Y]^2Var(X_2) + \\
& \ \ \ \ Var(X_2)Var(Y) + 2Cov(YX_1, (1-Y)X_2) \\
&= 0 + p^2\sigma_1^2+ p(1-p)\sigma_1^2 + 0 + (1-p)^2\sigma_2^2 + \sigma_2^2 p(1-p) + 2Cov(YX_1, (1-Y)X_2)\\
\end{align*}
Consider the Covariance using the formula $Cov(X, Y) = E[XY] - E[X]E[Y]$.
\begin{align*}
Cov(YX_1, (1-Y)X_2) &= E[YX_1(1-Y)X_2] - E[YX_1]E[(1-Y)X_2]\\
&= E[Y]E[X_1]E[X_2] - E[Y^2]E[X_1]E[X_2] - E[Y]E[X_1]E[X_2](1-E[Y])\\
&= 0
\end{align*}
Plug back in to Variance of Z:
\begin{align*}
Var(Z) &= p^2\sigma_1^2+ p(1-p)\sigma_1^2 + (1-p)^2\sigma_2^2 + \sigma_2^2 p(1-p)\\
&= \sigma_1^2 p + \sigma^2_2 (1-p)\\
\end{align*}
Thus, the expectation and variance of $Z$ are: $E[Z] = 0$ and $Var(Z) = \sigma_1^2 p + \sigma^2_2 (1-p)$.

\subsection*{b)}
Use rnorm() and rbern() function to generate some data that follows the pattern of $Z$ and a normal distribution with same mean and variance with $Z$.  
Then plot the probability density function and the cumulative distribution of both distributions:  
```{r, echo = T}
set.seed(7)
X_1 = rnorm(10000, 0, 10)
X_2 = rnorm(10000, 0, 50)
Y = rbern(10000, 0.4)
Z_norm = rnorm(10000, 0, sqrt(10^2*0.4 + 50^2 * 0.6))
Z = Y*X_1 + X_2 - Y*X_2
Z = sort(Z)
Z = data.frame(cbind(Index = c(1:10000), Z = Z))
Z_norm = sort(Z_norm)
Z_norm = data.frame(Z_norm = Z_norm)
Z_Data = cbind(Z, Z_norm)

ggplot() +
  geom_line(data = Z_Data, aes(x = Z, y = Index, col = 'Mixture Model'), size = 1) + 
  geom_line(data = Z_Data, aes(x = Z_norm, y = Index, col = 'Normal'), size = 1) + 
  labs(y = "Index", x = "Value of Data", title = "CDF of both Distributions") + 
  scale_color_manual(name = "Color", values = c("Mixture Model" = "red", "Normal" = "cyan"))

ggplot() + 
  geom_density(aes(x = Z_Data$Z, col = 'Mixture Model')) + 
  geom_density(aes(x = Z_Data$Z_norm, col = "Normal")) + 
  scale_color_manual(name = "Color", values = c("Mixture Model" = "red", "Normal" = "cyan")) + 
  labs(y = "Density", x = "Value of Data", title = "Density function of both distributions")
```
\newline
It is easy to notice from the above graphs that the Mixture Model has heavier tail than the normal distribution with same mean and variance.  
\newline
I have tries to change the value of $p$ from $0.001$ to $0.999$. I have noticed that whenever the value $p$ is closer to the end point of interval$(0,1)$, the two plots are getting closer to each other. i.e. that misture model can be regarded as a normal distribution, but to be honest it is still heavy tailed. Example of $p = 0.999$  and $p = 0.001$ graphs are as follows:   
```{r, echo = F}
set.seed(7)
X_1 = rnorm(10000, 0, 10)
X_2 = rnorm(10000, 0, 50)
Y = rbern(10000, 0.999)
Z_norm = rnorm(10000, 0, sqrt(10^2*0.999 + 50^2 * 0.001))
Z = Y*X_1 + X_2 - Y*X_2
Z = sort(Z)
Z = data.frame(cbind(Index = c(1:10000), Z = Z))
Z_norm = sort(Z_norm)
Z_norm = data.frame(Z_norm = Z_norm)
Z_Data = cbind(Z, Z_norm)

ggplot() + 
  geom_density(aes(x = Z_Data$Z, col = 'Mixture Model')) + 
  geom_density(aes(x = Z_Data$Z_norm, col = "Normal")) + 
  scale_color_manual(name = "Color", values = c("Mixture Model" = "red", "Normal" = "cyan")) + 
  labs(y = "Density", x = "Value of Data", title = "Density function of both distributions WITH p = 0.999")

set.seed(7)
X_1 = rnorm(10000, 0, 10)
X_2 = rnorm(10000, 0, 50)
Y = rbern(10000, 0.001)
Z_norm = rnorm(10000, 0, sqrt(10^2*0.001 + 50^2 * 0.999))
Z = Y*X_1 + X_2 - Y*X_2
Z = sort(Z)
Z = data.frame(cbind(Index = c(1:10000), Z = Z))
Z_norm = sort(Z_norm)
Z_norm = data.frame(Z_norm = Z_norm)
Z_Data = cbind(Z, Z_norm)

ggplot() + 
  geom_density(aes(x = Z_Data$Z, col = 'Mixture Model')) + 
  geom_density(aes(x = Z_Data$Z_norm, col = "Normal")) + 
  scale_color_manual(name = "Color", values = c("Mixture Model" = "red", "Normal" = "cyan")) + 
  labs(y = "Density", x = "Value of Data", title = "Density function of both distributions WITH p = 0.001")
```

